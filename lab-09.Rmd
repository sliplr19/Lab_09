---
title: "Lab 09 - Grading the professor, Pt. 1"
author: "Lindley Slipetz"
date: "03/30/2021"
output: github_document
---

### Load packages and data

```{r load-packages, message=FALSE, warning = FALSE}
#install.packages("vctrs")
#install.packages("tibble")
#install.packages("tidyverse")
library(tidyverse) 
#install.packages("broom")
#install.packages("tidymodels")
#install.packages("ggplot2")
library(tidymodels)
library(openintro)
```

Okay, loading tidymodels ended up being kind of a nightmare, but now we're ready to go.

### Exercise 1

```{r score_bar}
ggplot(evals, aes(x=factor(score)))+
  geom_bar(stat="count", width=0.7, fill="black")+
  labs(x = "Score", y = "Count", title = "Distribution of teaching evaluation scores") +
  theme(axis.text.x = element_text(angle = 90))
```

```{r score_sum}
evals %>%
  summarise(averageRating = mean(score),
             sdRating = sd(score))
```

The distribution is positively skewed. Students tend to rate their courses highly. The average score is 4.17 with a standard deviation of 0.54. I'm actually a bit surprised by this, but I would explain it by my perception of the gender disparity in evaluations (of course, I'm going to evaluate that later, so maybe this will change). Men tend to get high scores and there's more male professors than female professors, so that would skew the distribution.

### Exercise 2

```{r scatter_1}
ggplot(evals, aes(x= bty_avg, y=score)) +
  geom_point() +
  labs(x = "Evaluation Score", y = "Beauty Score", title = "Evaluation score vs beauty score") 
```

While it does seem true that, generally, higher beauty score is associated with higher evaluations (because the higher beauty scores are associated with higher evaluations), it's not a strong
association. 

### Exercise 3

```{r jitter_1}
ggplot(evals, aes(x=bty_avg, y=score)) +
  geom_jitter() +
  labs(x = "Evaluation Score", y = "Beauty Score", title = "Evaluation score vs beauty score") 
```

Jitter makes it so the points aren't in straight lines. It makes it more clear that there might be a linear relationship between evaluation score and beauty score.

### Exercise 4

```{r linear_reg_1}
lm_model <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')
lm_fit <- lm_model %>% 
          fit(score ~ bty_avg, data = evals)
lm_fit
```
**Regression model**: score = 0.6664bty_avg + 3.88034


###Exercise 5

```{r lin_reg_graph}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_jitter() +
  geom_smooth(method=lm, se=FALSE, color="orange") +
  labs(x = "Evaluation Score", y = "Beauty Score", title = "Evaluation score vs beauty score") 
```


### Exercise 6

A one unit change in score corresponds to a 0.6664 unit change in beauty score average.

### Exercise 7 

The mean of beauty score is 3.88034 when evaluation score is zero. I don't think this value is realistic in practice. I don't think someone would ever teach so bad as to get 0 from every student. 

### Exercise 8
```{r R_2}
glance(lm_fit$fit)
```
$R^2$ = 0.035, so the model explains 3.5% of the variance. It also tells us that the residuals are quite large.

### Exercise 9 


```{r linear_reg_2}
m_gen <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')
m_gen_fit <- m_gen %>% 
          fit(score ~ gender, data = evals)
m_gen_fit
```
The average difference between male and female scores is 0.1415. Female is associated with a score of 4.0928.

### Exercise 10

The equation for females is score = 4.0928 + 0.1415gender. The equation for male is score = 4.2343 + 0.1415gender.

### Exercise 11 

```{r linear_reg_3}
m_rank <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')
m_rank_fit <- m_rank %>% 
          fit(score ~ rank, data = evals)
m_rank_fit
```

The equation is score = 4.2843 - 0.1297rank_TT - 0.1452rank_T. The average score for teaching is 4.2843. As TT rank increases by one rank, holding T constant, scores decrease by 0.1297. As T rank increases by one rank, holding TT constant, scores decrease by 0.1452.

### Exercise 12

```{r rank_relevel}
evals <- evals %>%
  mutate(rank_relevel = case_when(
    rank == "teaching" ~ -1,
    rank == "tenure track" ~ 0,
    rank == "tenured" ~ 1
  ))
```

### Exercise 13


```{r linear_reg_4}
m_rank_relevel <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')
m_rank_relevel_fit <- m_rank_relevel %>% 
          fit(score ~ rank_relevel, data = evals)
m_rank_relevel_fit
glance(m_rank_relevel_fit$fit)
```

The equation is score = 4.19626 - 0.06601rank_relevel. When rank is tenure track, the average score is 4.19626. Being tenured is associated with lower scores. R^2 = 0.00975, so the model explains .975% of the variance. Again we see the residuals are large.

### Exercise 14


```{r tenure_eligible}
evals <- evals %>%
  mutate(tenure_eligible = case_when(
    rank == "teaching" ~ "no",
    rank == "tenure track" ~ "yes",
    rank == "tenured" ~ "yes"
  ))
```

### Exercise 15

Fit a new linear model called m_tenure_eligible to predict average professor evaluation score based on tenure_eligibleness of the professor. This is the new (regrouped) variable you created in Exercise 15. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the  
R2 of the model.

```{r linear_reg_5}
m_tenure_eligible <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')
m_tenure_eligible_fit <- m_tenure_eligible %>% 
          fit(score ~ tenure_eligible, data = evals)
m_tenure_eligible_fit
glance(m_tenure_eligible_fit$fit)
```
The equation is score = 4.2843 - 0.1405tenure_eligible. For teaching faculty, the average score is 4.2843. Being tenure eligible is associated with a 0.1405 decrease in score. R^2 = 0.0115, meaning the model explains 1.15% of the variance and again there are large residuals.
